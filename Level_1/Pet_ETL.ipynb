{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pet_ETL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWQs+WRAbTs4O8Q+FTHhm4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cekiorpes/big-data-challenge/blob/main/Level_1/Pet_ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WI7EMP077nzU",
        "outputId": "ae3e4c72-eac8-4b31-fedb-7f841a28035a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "spark_version = 'spark-3.0.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjpSVfOP7vuZ",
        "outputId": "22b81995-b3dc-464e-e8f9-17def21d0039"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-19 02:13:41--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 914037 (893K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.9.jar.1’\n",
            "\n",
            "postgresql-42.2.9.j 100%[===================>] 892.61K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2022-05-19 02:13:42 (10.5 MB/s) - ‘postgresql-42.2.9.jar.1’ saved [914037/914037]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"PetCloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ],
      "metadata": {
        "id": "OVQ1YXKT7yQw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "# Load in review data from S3 into a DataFrame\n",
        "url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Pet_Products_v1_00.tsv.gz\"\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "df = spark.read.option('header', 'true').csv(SparkFiles.get(\"amazon_reviews_us_Pet_Products_v1_00.tsv.gz\"), inferSchema=True, sep='\\t', header=True, timestampFormat=\"yyyy-mm-dd\")\n",
        "df.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QCVgk7e74fV",
        "outputId": "d10e8089-f799-4452-a632-5fb41c381f41"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
            "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|        review_date|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
            "|         US|   28794885| REAKC26P07MDN|B00Q0K9604|     510387886|(8-Pack) EZwhelp ...|    Pet Products|          5|            0|          0|   N|                Y|A great purchase ...|Best belly bands ...|2015-01-31 00:08:00|\n",
            "|         US|   11488901|R3NU7OMZ4HQIEG|B00MBW5O9W|     912374672|Warren Eckstein's...|    Pet Products|          2|            0|          1|   N|                Y|My dogs love Hugs...|My dogs love Hugs...|2015-01-31 00:08:00|\n",
            "|         US|   43214993|R14QJW3XF8QO1P|B0084OHUIO|     902215727|Tyson's True Chew...|    Pet Products|          5|            0|          0|   N|                Y|I have been purch...|I have been purch...|2015-01-31 00:08:00|\n",
            "|         US|   12835065|R2HB7AX0394ZGY|B001GS71K2|     568880110|Soft Side Pet Cra...|    Pet Products|          5|            0|          0|   N|                Y|it is easy to ope...|It is extremely w...|2015-01-31 00:08:00|\n",
            "|         US|   26334022| RGKMPDQGSAHR3|B004ABH1LG|     692846826|EliteField 3-Door...|    Pet Products|          5|            0|          0|   N|                Y|           Dog crate|Worked really wel...|2015-01-31 00:08:00|\n",
            "|         US|   22283621|R1DJCVPQGCV66E|B00AX0LFM4|     590674141|Carlson 68-Inch W...|    Pet Products|          5|            0|          0|   N|                Y|          Five Stars|I love my gates! ...|2015-01-31 00:08:00|\n",
            "|         US|   14469895|R3V52EAWLPBFQG|B00DQFZGZ0|     688538603|Dog Seat Cover Wi...|    Pet Products|          3|            0|          0|   N|                Y|Seat belt tugs on...|Didn't quite work...|2015-01-31 00:08:00|\n",
            "|         US|   50896354|R3DKO8J1J28QBI|B00DIRF9US|     742358789|The Bird Catcher ...|    Pet Products|          2|            0|          0|   N|                Y|Great Pole, but S...|I had the origina...|2015-01-31 00:08:00|\n",
            "|         US|   18440567| R764DBXGRNECG|B00JRCBFUG|     869798483|Cat Bed - Purrfec...|    Pet Products|          5|            1|          1|   N|                N|     My cat loves it|The pad is very s...|2015-01-31 00:08:00|\n",
            "|         US|   50502362| RW1853GAT0Z9F|B000L3XYZ4|     501118658|PetSafe Drinkwell...|    Pet Products|          5|            0|          0|   N|                Y|          Five Stars|My cat drinks mor...|2015-01-31 00:08:00|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.count())\n",
        "df = df.dropna()\n",
        "print(df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ni-WTVIx8Kwa",
        "outputId": "cadeac30-8339-4718-f5cd-a1d52948f7e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2643619\n",
            "2643241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPiWRk8o8N-r",
        "outputId": "9fa6c3f0-cf5a-42a3-8f95-4e0b122c1249"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- marketplace: string (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- review_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- product_parent: integer (nullable = true)\n",
            " |-- product_title: string (nullable = true)\n",
            " |-- product_category: string (nullable = true)\n",
            " |-- star_rating: integer (nullable = true)\n",
            " |-- helpful_votes: integer (nullable = true)\n",
            " |-- total_votes: integer (nullable = true)\n",
            " |-- vine: string (nullable = true)\n",
            " |-- verified_purchase: string (nullable = true)\n",
            " |-- review_headline: string (nullable = true)\n",
            " |-- review_body: string (nullable = true)\n",
            " |-- review_date: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform data for review_id_table"
      ],
      "metadata": {
        "id": "pZ5YKGM89m6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new DataFrame for review_id_table\n",
        "review_id_table = df.select([\"review_id\", \"customer_id\", \"product_id\", \"product_parent\", \"review_date\"])\n",
        "review_id_table.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa9nIXhe8SjF",
        "outputId": "4d1c682e-2dc4-4adf-bb2e-8097bcc2798b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------+--------------+-------------------+\n",
            "|     review_id|customer_id|product_id|product_parent|        review_date|\n",
            "+--------------+-----------+----------+--------------+-------------------+\n",
            "| REAKC26P07MDN|   28794885|B00Q0K9604|     510387886|2015-01-31 00:08:00|\n",
            "|R3NU7OMZ4HQIEG|   11488901|B00MBW5O9W|     912374672|2015-01-31 00:08:00|\n",
            "|R14QJW3XF8QO1P|   43214993|B0084OHUIO|     902215727|2015-01-31 00:08:00|\n",
            "|R2HB7AX0394ZGY|   12835065|B001GS71K2|     568880110|2015-01-31 00:08:00|\n",
            "| RGKMPDQGSAHR3|   26334022|B004ABH1LG|     692846826|2015-01-31 00:08:00|\n",
            "|R1DJCVPQGCV66E|   22283621|B00AX0LFM4|     590674141|2015-01-31 00:08:00|\n",
            "|R3V52EAWLPBFQG|   14469895|B00DQFZGZ0|     688538603|2015-01-31 00:08:00|\n",
            "|R3DKO8J1J28QBI|   50896354|B00DIRF9US|     742358789|2015-01-31 00:08:00|\n",
            "| R764DBXGRNECG|   18440567|B00JRCBFUG|     869798483|2015-01-31 00:08:00|\n",
            "| RW1853GAT0Z9F|   50502362|B000L3XYZ4|     501118658|2015-01-31 00:08:00|\n",
            "+--------------+-----------+----------+--------------+-------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write DataFrame to RDS\n",
        "mode=\"append\"\n",
        "jdbc_url = \"jdbc:postgresql://mypostgresdb.cojhn4lpsels.us-east-1.rds.amazonaws.com:5432/big_data_db\"\n",
        "config = {\"user\":\"root\",\n",
        "          \"password\": \"\",\n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ],
      "metadata": {
        "id": "hVY1BkK18TeT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_id_table.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)"
      ],
      "metadata": {
        "id": "Oh74blhO8ZeR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform data for products table"
      ],
      "metadata": {
        "id": "zTgK8yIj91Bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new DataFrame for products table\n",
        "products = df.select([\"product_id\", \"product_title\"])\n",
        "products.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf9wJEmy8c6n",
        "outputId": "57a1c669-63a3-425f-81c2-5dd92b741f1d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+\n",
            "|product_id|       product_title|\n",
            "+----------+--------------------+\n",
            "|B00Q0K9604|(8-Pack) EZwhelp ...|\n",
            "|B00MBW5O9W|Warren Eckstein's...|\n",
            "|B0084OHUIO|Tyson's True Chew...|\n",
            "|B001GS71K2|Soft Side Pet Cra...|\n",
            "|B004ABH1LG|EliteField 3-Door...|\n",
            "|B00AX0LFM4|Carlson 68-Inch W...|\n",
            "|B00DQFZGZ0|Dog Seat Cover Wi...|\n",
            "|B00DIRF9US|The Bird Catcher ...|\n",
            "|B00JRCBFUG|Cat Bed - Purrfec...|\n",
            "|B000L3XYZ4|PetSafe Drinkwell...|\n",
            "+----------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop duplicates\n",
        "products = products.dropDuplicates()"
      ],
      "metadata": {
        "id": "Z3NxRCOI8iy_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write DataFrame to RDS\n",
        "products.write.jdbc(url=jdbc_url, table='products', mode=mode, properties=config)"
      ],
      "metadata": {
        "id": "hle3nSNy9F3e"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform data for customers table"
      ],
      "metadata": {
        "id": "uggFXLcq95Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new DataFrame for customers table\n",
        "customers = df.select([\"customer_id\"])\n",
        "customers.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFsHXaGK9JKq",
        "outputId": "0229418b-e781-4e5e-cc6a-325ae751894b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|customer_id|\n",
            "+-----------+\n",
            "|   28794885|\n",
            "|   11488901|\n",
            "|   43214993|\n",
            "|   12835065|\n",
            "|   26334022|\n",
            "+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Count ratings by customer_id\n",
        "customers = df.groupby(\"customer_id\").agg({\"customer_id\":\"count\"})\n",
        "customers.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgBpq6599W0d",
        "outputId": "ecee6f6a-1dcb-4980-9827-fb7d46a0796f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------+\n",
            "|customer_id|count(customer_id)|\n",
            "+-----------+------------------+\n",
            "|   10270641|                 1|\n",
            "|   18365872|                 1|\n",
            "|   16711087|                 1|\n",
            "|   10742726|                 2|\n",
            "|   41169638|                 1|\n",
            "+-----------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename count column\n",
        "customers = customers.withColumnRenamed(\"count(customer_id)\", \"customer_count\")\n",
        "customers.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3yafMfd9YmF",
        "outputId": "06c48c6e-75c1-47d8-bbfe-7e933dd1d7ea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "|customer_id|customer_count|\n",
            "+-----------+--------------+\n",
            "|   10270641|             1|\n",
            "|   18365872|             1|\n",
            "|   16711087|             1|\n",
            "|   10742726|             2|\n",
            "|   41169638|             1|\n",
            "+-----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customers = customers.filter((customers.customer_id != 42319435) & (customers.customer_id != 36605391) & (customers.customer_id != 50724762) & (customers.customer_id != 50753128))"
      ],
      "metadata": {
        "id": "KOEgCCiuMzSr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Write DataFrame to RDS\n",
        "from py4j.protocol import Py4JJavaError\n",
        "\n",
        "try:\n",
        "  customers.write.jdbc(url=jdbc_url, table='customers', mode=mode, properties=config)\n",
        "\n",
        "except Py4JJavaError as err:\n",
        "  print(str(err))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iZl33r-X9bTT",
        "outputId": "b04ffb09-cebb-4d7d-e6e3-ffca1453688a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred while calling o171.jdbc.\n",
            ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 28.0 failed 1 times, most recent failure: Lost task 1.0 in stage 28.0 (TID 236, 46684ff6f4e5, executor driver): java.sql.BatchUpdateException: Batch entry 52 INSERT INTO customers (\"customer_id\",\"customer_count\") VALUES (12160854,3) was aborted: ERROR: duplicate key value violates unique constraint \"customers_pkey\"\n",
            "  Detail: Key (customer_id)=(12160854) already exists.  Call getNextException to see other errors in the batch.\n",
            "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:153)\n",
            "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n",
            "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1357)\n",
            "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1382)\n",
            "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:494)\n",
            "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n",
            "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n",
            "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:699)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:871)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:869)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:994)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:994)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:748)\n",
            "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"customers_pkey\"\n",
            "  Detail: Key (customer_id)=(12160854) already exists.\n",
            "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n",
            "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n",
            "\t... 20 more\n",
            "\n",
            "Driver stacktrace:\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
            "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
            "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n",
            "\tat scala.Option.foreach(Option.scala:407)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n",
            "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n",
            "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2114)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2135)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2154)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2179)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:994)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n",
            "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:992)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:869)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n",
            "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
            "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
            "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
            "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:767)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:398)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:790)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
            "\tat java.lang.Thread.run(Thread.java:748)\n",
            "Caused by: java.sql.BatchUpdateException: Batch entry 52 INSERT INTO customers (\"customer_id\",\"customer_count\") VALUES (12160854,3) was aborted: ERROR: duplicate key value violates unique constraint \"customers_pkey\"\n",
            "  Detail: Key (customer_id)=(12160854) already exists.  Call getNextException to see other errors in the batch.\n",
            "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:153)\n",
            "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n",
            "\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1357)\n",
            "\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1382)\n",
            "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:494)\n",
            "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n",
            "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n",
            "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:699)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:871)\n",
            "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:869)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:994)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:994)\n",
            "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2154)\n",
            "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
            "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:463)\n",
            "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
            "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:466)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\t... 1 more\n",
            "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"customers_pkey\"\n",
            "  Detail: Key (customer_id)=(12160854) already exists.\n",
            "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n",
            "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n",
            "\t... 20 more\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform data for vine_table"
      ],
      "metadata": {
        "id": "Z4gTbX_398mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new DataFrame for vine_table\n",
        "vine_table = df.select([\"review_id\", \"star_rating\", \"helpful_votes\", \"total_votes\", \"vine\"])\n",
        "vine_table.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBalH0kB9eQY",
        "outputId": "797fd1ad-cc4f-4025-8c42-7d5d2a332fb4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+-------------+-----------+----+\n",
            "|     review_id|star_rating|helpful_votes|total_votes|vine|\n",
            "+--------------+-----------+-------------+-----------+----+\n",
            "| REAKC26P07MDN|          5|            0|          0|   N|\n",
            "|R3NU7OMZ4HQIEG|          2|            0|          1|   N|\n",
            "|R14QJW3XF8QO1P|          5|            0|          0|   N|\n",
            "|R2HB7AX0394ZGY|          5|            0|          0|   N|\n",
            "| RGKMPDQGSAHR3|          5|            0|          0|   N|\n",
            "|R1DJCVPQGCV66E|          5|            0|          0|   N|\n",
            "|R3V52EAWLPBFQG|          3|            0|          0|   N|\n",
            "|R3DKO8J1J28QBI|          2|            0|          0|   N|\n",
            "| R764DBXGRNECG|          5|            1|          1|   N|\n",
            "| RW1853GAT0Z9F|          5|            0|          0|   N|\n",
            "+--------------+-----------+-------------+-----------+----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write DataFrame to RDS\n",
        "vine_table.write.jdbc(url=jdbc_url, table='vine_table', mode=mode, properties=config)"
      ],
      "metadata": {
        "id": "7PVUg5ih9jGj"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}